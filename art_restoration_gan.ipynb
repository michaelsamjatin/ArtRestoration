{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GENERATOR\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_filters=64):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = self._block(in_channels, num_filters)\n",
    "        self.encoder2 = self._block(num_filters, num_filters * 2)\n",
    "        self.encoder3 = self._block(num_filters * 2, num_filters * 4)\n",
    "        self.encoder4 = self._block(num_filters * 4, num_filters * 8)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self._block(num_filters * 8, num_filters * 16, dilation=2)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        self.decoder1 = self._block(num_filters * 16 + num_filters * 8, num_filters * 8, dilation=2)\n",
    "        self.decoder2 = self._block(num_filters * 8 + num_filters * 4, num_filters * 4)\n",
    "        self.decoder3 = self._block(num_filters * 4 + num_filters * 2, num_filters * 2)\n",
    "        self.decoder4 = self._block(num_filters * 2 + num_filters, num_filters)\n",
    "\n",
    "        # Final layer\n",
    "        self.final = nn.Conv2d(num_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def _block(self, in_channels, out_channels, dilation=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(nn.MaxPool2d(2)(e1))\n",
    "        e3 = self.encoder3(nn.MaxPool2d(2)(e2))\n",
    "        e4 = self.encoder4(nn.MaxPool2d(2)(e3))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(nn.MaxPool2d(2)(e4))\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d1 = self.decoder1(torch.cat([nn.Upsample(scale_factor=2)(bottleneck), e4], dim=1))\n",
    "        d2 = self.decoder2(torch.cat([nn.Upsample(scale_factor=2)(d1), e3], dim=1))\n",
    "        d3 = self.decoder3(torch.cat([nn.Upsample(scale_factor=2)(d2), e2], dim=1))\n",
    "        d4 = self.decoder4(torch.cat([nn.Upsample(scale_factor=2)(d3), e1], dim=1))\n",
    "\n",
    "        # Final layer\n",
    "        return torch.tanh(self.final(d4))\n",
    "    \n",
    "# DISCRIMINATOR\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_filters=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, num_filters, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters, num_filters * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_filters * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_filters * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=4, stride=2, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(num_filters * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters * 8, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_filters=64):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, num_filters, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters, num_filters * 2, kernel_size=4, stride=2, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(num_filters * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_filters * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=4, stride=1, padding=1),  # No stride\n",
    "            nn.BatchNorm2d(num_filters * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(num_filters * 8, 1, kernel_size=4, stride=1, padding=1),  # Output a matrix\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ArtRestorationDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hdf5_file (str): Path to the HDF5 file.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.transform = transform\n",
    "        # Open the HDF5 file and keep it open\n",
    "        # self.hdf = h5py.File(self.hdf5_file, \"r\")\n",
    "        # self.damaged_images = self.hdf[\"damaged_paintings\"]\n",
    "        # self.ground_truth_images = self.hdf[\"initial\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.hdf5_file,'r') as hdf:\n",
    "            lens = len(hdf[\"damaged_paintings\"])\n",
    "        return lens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images from HDF5 file\n",
    "        with h5py.File(self.hdf5_file,'r') as hdf:\n",
    "            damaged_img = hdf[\"damaged_paintings\"][idx]\n",
    "            ground_truth_img = hdf[\"initial\"][idx]\n",
    "\n",
    "        # Convert numpy arrays to PIL Images (optional, depending on your transforms)\n",
    "        damaged_img = transforms.ToPILImage()(damaged_img)\n",
    "        ground_truth_img = transforms.ToPILImage()(ground_truth_img)\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            damaged_img = self.transform(damaged_img)\n",
    "            ground_truth_img = self.transform(ground_truth_img)\n",
    "\n",
    "        return damaged_img, ground_truth_img\n",
    "        \n",
    "    # def __del__(self):\n",
    "    #     # Close the HDF5 file when the dataset is deleted\n",
    "    #     if hasattr(self, \"hdf\"):\n",
    "    #         self.hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(size_bytes):\n",
    "    \"\"\"\n",
    "    Convert bytes to a human-readable format (KB, MB, GB, etc.).\n",
    "    \"\"\"\n",
    "    # Define the size units\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"]\n",
    "    \n",
    "    # Determine the appropriate unit\n",
    "    unit_index = 0\n",
    "    while size_bytes >= 1024 and unit_index < len(units) - 1:\n",
    "        size_bytes /= 1024\n",
    "        unit_index += 1\n",
    "    \n",
    "    # Return the formatted string\n",
    "    return f\"{size_bytes:.2f} {units[unit_index]}\"\n",
    "\n",
    "def estimate_vram_usage(batch_size=16, scale=128, channels=3, bytes=4): \n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    generator_params = count_parameters(UNetGenerator())\n",
    "    global_discriminator_params = count_parameters(Discriminator())\n",
    "    local_discriminator_params = count_parameters(Discriminator())\n",
    "\n",
    "    total_params = generator_params + global_discriminator_params + local_discriminator_params\n",
    "    memory_params = total_params * bytes\n",
    "    memory_act = batch_size * scale**2 * 2_560 * 10\n",
    "    memory_grad = total_params * bytes\n",
    "    memory_opt = 2 * total_params * bytes\n",
    "    memory_data = batch_size * channels * scale**2 * bytes\n",
    "\n",
    "    total_memory = memory_params + memory_act + memory_grad + memory_opt + memory_data\n",
    "\n",
    "    return total_memory\n",
    "\n",
    "print(f\"Total VRAM: {convert_bytes(estimate_vram_usage())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch_directml\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def training(scale=128, \n",
    "            batch_size=16, \n",
    "            num_epochs=100, \n",
    "            dataset_path=\"./dataset_2.hdf5\", \n",
    "            checkpoint_dir = \"./checkpoints\", \n",
    "            device=torch_directml.device(), \n",
    "            steps = 50,\n",
    "            gan_lr=0.0002, \n",
    "            gan_betas=(0.5, 0.999), \n",
    "            dg_lr=0.0002, \n",
    "            dg_betas=(0.5, 0.999), \n",
    "            dl_lr=0.0002,\n",
    "            dl_betas=(0.5, 0.999),\n",
    "            save_old_checkpoints=False\n",
    "             ):\n",
    "    \n",
    "    # Adversarial loss (Binary Cross-Entropy)\n",
    "    adversarial_loss = nn.BCELoss().to(device)\n",
    "    # adversarial_loss = nn.MSELoss().to(device)\n",
    "\n",
    "    # Reconstruction loss (L1 loss)\n",
    "    reconstruction_loss = nn.L1Loss().to(device)\n",
    "\n",
    "    # Initialize models\n",
    "    generator = UNetGenerator().to(device)\n",
    "    global_discriminator = Discriminator().to(device)\n",
    "    local_discriminator = PatchGANDiscriminator().to(device)\n",
    "    torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(global_discriminator.parameters(), max_norm=1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(local_discriminator.parameters(), max_norm=1.0)\n",
    "\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    generator.apply(weights_init)\n",
    "    global_discriminator.apply(weights_init)\n",
    "    local_discriminator.apply(weights_init)\n",
    "\n",
    "    dummy_input = torch.randn(batch_size, 3, scale, scale).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_global = global_discriminator(dummy_input)\n",
    "        output_local = local_discriminator(dummy_input)\n",
    "\n",
    "    dg_ouput_shape = output_global.shape[2:]\n",
    "    dl_ouput_shape = output_local.shape[2:]\n",
    "\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=gan_lr, betas=gan_betas)\n",
    "    optimizer_D_global = torch.optim.Adam(global_discriminator.parameters(), lr=dg_lr, betas=dg_betas)\n",
    "    optimizer_D_local = torch.optim.Adam(local_discriminator.parameters(), lr=dl_lr, betas=dl_betas)\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((scale, scale)),\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = ArtRestorationDataset(hdf5_file=dataset_path, transform=transform)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Define checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Function to find the latest checkpoint\n",
    "    def find_latest_checkpoint(checkpoint_dir, save_old_checkpoints=False):\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch_\") and f.endswith(\".pth\")]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        # Extract epoch numbers and find the latest one\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"_\")[2].split(\".\")[0]))\n",
    "\n",
    "        if len(checkpoints) > 1 and not save_old_checkpoints: \n",
    "            checkpoints.remove(latest_checkpoint)\n",
    "            lastfile = os.path.join(checkpoint_dir, checkpoints[0])\n",
    "            if os.path.exists(lastfile):\n",
    "                os.remove(lastfile)\n",
    "\n",
    "        return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "    if True: \n",
    "        # Find the latest checkpoint\n",
    "        latest_checkpoint = find_latest_checkpoint(checkpoint_dir, save_old_checkpoints)\n",
    "\n",
    "        start_epoch = 0\n",
    "\n",
    "        # Load the latest checkpoint if it exists\n",
    "        if latest_checkpoint:\n",
    "            print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "            checkpoint = torch.load(latest_checkpoint)\n",
    "            generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "            global_discriminator.load_state_dict(checkpoint[\"global_discriminator_state_dict\"])\n",
    "            local_discriminator.load_state_dict(checkpoint[\"local_discriminator_state_dict\"])\n",
    "            optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
    "            optimizer_D_global.load_state_dict(checkpoint[\"optimizer_D_global_state_dict\"])\n",
    "            optimizer_D_local.load_state_dict(checkpoint[\"optimizer_D_local_state_dict\"])\n",
    "            start_epoch = checkpoint[\"epoch\"]\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "        # Training loop with tqdm and checkpointing\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            # Wrap the dataloader with tqdm for progress visualization\n",
    "            progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            for i, (damaged_imgs, ground_truth_imgs) in progress_bar:\n",
    "                damaged_imgs: torch.Tensor = damaged_imgs.to(device)\n",
    "                ground_truth_imgs = ground_truth_imgs.to(device)\n",
    "\n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones(damaged_imgs.size(0), 1, *dg_ouput_shape).to(device)\n",
    "                fake = torch.zeros(damaged_imgs.size(0), 1, *dg_ouput_shape).to(device)\n",
    "                valid_local = torch.ones(damaged_imgs.size(0), 1, *dl_ouput_shape).to(device)\n",
    "                fake_local  = torch.zeros(damaged_imgs.size(0), 1, *dl_ouput_shape).to(device)\n",
    "\n",
    "                # if i < 50 or i&0:\n",
    "                # Train Discriminators\n",
    "                optimizer_D_global.zero_grad()\n",
    "                optimizer_D_local.zero_grad()\n",
    "                \n",
    "                # Generate restored images\n",
    "                restored_fake = generator(damaged_imgs)\n",
    "\n",
    "                # Global discriminator loss\n",
    "                real_loss_global = adversarial_loss(global_discriminator(ground_truth_imgs), valid)\n",
    "                fake_loss_global = adversarial_loss(global_discriminator(restored_fake.detach()), fake)\n",
    "                d_loss_global = (real_loss_global + fake_loss_global) / 2\n",
    "\n",
    "                # Local discriminator loss\n",
    "                real_loss_local = adversarial_loss(local_discriminator(ground_truth_imgs), valid_local)\n",
    "                fake_loss_local = adversarial_loss(local_discriminator(restored_fake.detach()), fake_local)\n",
    "                d_loss_local = (real_loss_local + fake_loss_local) / 2\n",
    "                \n",
    "                # Average the PatchGAN loss over all patches\n",
    "                d_loss_local_avg = d_loss_local.mean()\n",
    "\n",
    "                # Total discriminator loss\n",
    "                d_loss_global.backward()\n",
    "                d_loss_local.backward()\n",
    "                optimizer_D_global.step()\n",
    "                optimizer_D_local.step()\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                only_tot = i >= steps * 3\n",
    "                only_loc = i >= steps * 2 and not only_tot\n",
    "                only_glb = i >= steps * 1 and not only_loc\n",
    "                only_rec = i >= steps * 0 and not only_glb\n",
    "\n",
    "                # Adversarial loss\n",
    "                g_loss_adv_global = 0 if not only_glb else adversarial_loss(global_discriminator(restored_fake), valid)\n",
    "                g_loss_adv_local = 0 if not only_loc else adversarial_loss(local_discriminator(restored_fake), valid_local)\n",
    "\n",
    "                # Reconstruction loss\n",
    "                g_loss_rec = 0 if not only_rec else reconstruction_loss(restored_fake, ground_truth_imgs)\n",
    "\n",
    "                # Compute total generator loss\n",
    "                # g_loss = g_loss_adv_global + g_loss_adv_local\n",
    "                g_loss = g_loss_adv_global + g_loss_adv_local + g_loss_rec\n",
    "\n",
    "                # total_loss = g_loss_adv_global + g_loss_adv_local + g_loss_rec\n",
    "                # weight_adv_global = g_loss_adv_global / total_loss\n",
    "                # weight_adv_local = g_loss_adv_local / total_loss\n",
    "                # weight_rec = g_loss_rec / total_loss\n",
    "                # g_loss = weight_adv_global * g_loss_adv_global + weight_adv_local * g_loss_adv_local + weight_rec * g_loss_rec\n",
    "\n",
    "                # Backward pass with scaling\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Show loss progress\n",
    "                progress_bar.set_postfix({\n",
    "                    \"DG Loss\": d_loss_global.item(),\n",
    "                    \"DL Loss\": d_loss_local_avg.item(),\n",
    "                    \"G  Loss\": g_loss.item()\n",
    "                })\n",
    "\n",
    "            # Save checkpoints\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"generator_state_dict\": generator.state_dict(),\n",
    "                \"global_discriminator_state_dict\": global_discriminator.state_dict(),\n",
    "                \"local_discriminator_state_dict\": local_discriminator.state_dict(),\n",
    "                \"optimizer_G_state_dict\": optimizer_G.state_dict(),\n",
    "                \"optimizer_D_global_state_dict\": optimizer_D_global.state_dict(),\n",
    "                \"optimizer_D_local_state_dict\": optimizer_D_local.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "for steps in [30, 40, 50]:\n",
    "    # steps          = 20\n",
    "    trial          = f\"{steps}-step-warm-up\"\n",
    "    scale          = 64\n",
    "    batch_size     = 32\n",
    "    gan_lr         = 0.0002\n",
    "    gan_betas      = (0.5, 0.9)\n",
    "    dl_lr          = 0.0002\n",
    "    dl_betas       = (0.5, 0.9)\n",
    "    dg_lr          = 0.0002\n",
    "    dg_betas       = (0.5, 0.9)\n",
    "    num_epochs     = 400\n",
    "    dataset_scale  = scale\n",
    "    dataset_path   = f\"./dataset_{dataset_scale}.hdf5\"\n",
    "    checkpoint_dir = f\"trial_scale_{scale}_{trial}/\"\n",
    "    device = torch_directml.device()\n",
    "\n",
    "    training(\n",
    "        scale=scale,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        dataset_path=dataset_path,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        device=device,\n",
    "        steps=steps,\n",
    "        save_old_checkpoints=False\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained Generator\n",
    "checkpoint = torch.load(f\"./{checkpoint_dir}checkpoint_epoch_{num_epochs}.pth\")\n",
    "generator = UNetGenerator().to(device)  # Replace with your Generator class\n",
    "generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "generator.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    ([transforms.Resize((scale, scale))] * (dataset_scale != scale)) + # Only scale, if dataset_scale > scale\n",
    "    [transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ArtRestorationDataset(hdf5_file=dataset_path, transform=transform)  # Replace with your dataset\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Pick a random sample\n",
    "random_index = np.random.randint(0, len(dataset))\n",
    "damaged_img, ground_truth_img = dataset[1495]\n",
    "\n",
    "# Move to device and add batch dimension\n",
    "damaged_img = damaged_img.unsqueeze(0).to(device)\n",
    "\n",
    "# Generate restored image\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    restored_img = generator(damaged_img)\n",
    "\n",
    "# Convert tensors to numpy arrays for visualization\n",
    "ground_truth_img = ground_truth_img.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "damaged_img = damaged_img.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "restored_img = restored_img.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Clip and normalize images to [0, 1] range\n",
    "damaged_img = np.clip((damaged_img + 1) / 2, 0, 1)  # Assuming images are normalized to [-1, 1]\n",
    "ground_truth_img = np.clip((ground_truth_img + 1) / 2, 0, 1)\n",
    "restored_img = np.clip((restored_img + 1) / 2, 0, 1)\n",
    "\n",
    "with_upscaling = True\n",
    "\n",
    "# Upscale the restored image to the original size (e.g., 512x512)\n",
    "damaged_img_upscaled = np.array(Image.fromarray((damaged_img * 255).astype(np.uint8)).resize((512, 512), Image.BILINEAR)) / 255\n",
    "ground_truth_img_upscaled = np.array(Image.fromarray((ground_truth_img * 255).astype(np.uint8)).resize((512, 512), Image.BILINEAR)) / 255\n",
    "restored_img_upscaled = np.array(Image.fromarray((restored_img * 255).astype(np.uint8)).resize((512, 512), Image.BILINEAR)) / 255\n",
    "\n",
    "# Display images\n",
    "\n",
    "num_imgs = 5 if with_upscaling else 3\n",
    "print(f\"Model: {checkpoint_dir}checkpoint_epoch_{num_epochs} | Dataset: {dataset_path} | Sample Index: {random_index}\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, num_imgs, 1)\n",
    "plt.title(f\"Damaged Image ({scale}x{scale})\")\n",
    "plt.imshow(damaged_img)\n",
    "# plt.imshow(damaged_img_upscaled)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, num_imgs, 2)\n",
    "plt.title(f\"Restored Image ({scale}x{scale})\")\n",
    "# plt.imshow(restored_img_upscaled)\n",
    "plt.imshow(restored_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, num_imgs, 3)\n",
    "plt.title(f\"Ground Truth Image ({scale}x{scale})\")\n",
    "# plt.imshow(ground_truth_img_upscaled)\n",
    "plt.imshow(ground_truth_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
